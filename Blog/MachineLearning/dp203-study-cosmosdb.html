<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link href="/favicon.ico" rel="icon" type="image/x-icon">
    <title>DP-203 Study Guide part 2 - Cosmos DB - SixSidedDice.com - Blog</title>
    <link rel="stylesheet" href="https://bootswatch.com/4/slate/bootstrap.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/prismjs@1.25.0/themes/prism-okaidia.css" integrity="sha256-nwDipdLn93O1CZGoRDor0i4CLmDQb+mdg/yaYMUCuLM=" crossorigin="anonymous">
    <link rel="stylesheet" href="/css/site.css">
    <link rel="stylesheet" href="./../site.css">
    <script src="https://kit.fontawesome.com/d22effaf67.js" crossorigin="anonymous"></script>
    <script type="module" src="/js/header.js"></script>
    <script type="module" src="/js/footer.js"></script>
    
</head>
<body>
    <six-sided-header></six-sided-header>
    <div class="container-fluid">
        <main role="main" class="pb-3">
            
<div class="article-header">
    May 16, 2022

        <span class="badge badge-info">DP203</span>
        <span class="badge badge-info">Cosmos DB</span>
</div>

<h1 id="implementing-non-relational-data-stores">Implementing non-relational data stores</h1>
<p>The <a href="https://docs.microsoft.com/en-us/learn/certifications/azure-data-engineer/" target="_blank">DP-203: Azure Data Engineering Associate</a> exam requires subject matter expertise in Azure solutions for non-relational data storage. This section of the syllabus includes:</p>
<ul>
<li>Azure Storage</li>
<li>Cosmos DB</li>
<li>Azure Data Lake</li>
</ul>
<p>This part 2 guide will cover <a href="https://docs.microsoft.com/en-us/azure/cosmos-db/introduction" target="_blank">Cosmos DB</a>.</p>
<h2 id="introduction-to-cosmos-db">Introduction to Cosmos DB</h2>
<p>Building a highly scalable, globally distributed database is hard. 
In 2010, Microsoft realised they needed to build something very different to SQL server to handle global distribution needs.</p>
<p>In 2015, Microsoft released Azure Document DB, a distributed database that supported SQL queries over JSON documents.</p>
<p>In 2017, Azure Document DB was rebranded as Cosmos DB.</p>
<h2 id="features">Features</h2>
<ul>
<li>Cloud based NoSQL database / Database as a service (DaaS)</li>
<li>Turnkey globally distributed</li>
<li>Serverless architecture</li>
<li>No operational overhead</li>
<li>No schema or index management (=&gt; no application downtime while schema is migrated)</li>
<li>Multi-model (Key value, Json documents, graph, and table/columnar data store)</li>
<li>Multi-language APIs (Java, .NET, Python, Node.js, JavaScript etc.)</li>
<li>Highly available, reliable and secure (always on, 99.999% uptime, &lt; 10ms latency)</li>
<li>Unlimited scale for both storage and throughput.</li>
<li>Supports configurable consistency levels: strong, eventually, consistent prefix, session and bounded-staleness.</li>
</ul>
<h2 id="use-cases">Use cases</h2>
<ul>
<li>IoT</li>
<li>Retail and marketing</li>
<li>Gaming</li>
<li>Web and mobile applications</li>
</ul>
<h2 id="multi-model-support">Multi-model support</h2>
<p>Cosmos DB supports data storage in a number of model. Each model has its own API</p>
<ul>
<li>Key-value -&gt; Table API</li>
<li>Wide-column (table/columnar) -&gt; Cassandra API</li>
<li>Graph -&gt; Gremlin API</li>
<li>Document -&gt; Core(SQL) API and MongoDB API</li>
</ul>
<h3 id="document-apis">Document APIs</h3>
<p>The Core(SQL) API is the original Document DB platform API and support storage of Json documents. 
The SQL API is also the only API that supports a SQL-like query language and a server-side programming model for transactional stored procedures.</p>
<p><code>Exam tip: any question references a SQL-like query language then Core(SQL) API will be the answer.</code></p>
<p>MongoDB API is an API that is fully compatible with MongoDB (implementing the same wire protocol and supporting storage of documents in BSON format).
This API allows migration of existing MongoDB solutions to Cosmos DB with minimal changes to the connection string.</p>
<p>Microsoft advise that the Core(SQL) API should be used for new developments.</p>
<p><code>Note: Although both the Core(SQL) and MongoDB APIs are Document-oriented they store data in different formats and cannot both be used on the same database.</code></p>
<h3 id="cosmos-db-table-api">Cosmos DB Table API</h3>
<p>The Cosmos DB Table API provides a key-value store API that supported the same protocol as Azure Table Storage API. It can be viewed as a premium offering for this API.
Existing customers using Azure Table Storage can migrate to Cosmos DB using the Table API.
Row values can be simple value like a string or number.
Rows cannot store objects/documents.</p>
<h3 id="cosmos-db-cassandra-api">Cosmos DB Cassandra API</h3>
<p>Cassandra is a wide column NoSQL database. The Cosmos DB Cassandra API supports the same wire protocol as Cassandra allowing simple migrations from
Cassandra to Cosmos DB and the use of Cassandra tools such as Data Explorer and SDKs such as the Cassandra CSharp Driver.</p>
<h3 id="cosmos-db-gremlin-api">Cosmos DB Gremlin API</h3>
<p>The Cosmos DB Gremlin API supports the Gremlin Graph data model. Again, the Gremlin wire protocol is supported (allowing easy migration from Gremlin to Cosmos DB).
Efficient for graph traversal (e.g. friends in social networks, device connections in IoT solution, etc.)</p>
<h2 id="provisioning-a-cosmos-db-account">Provisioning a Cosmos DB Account</h2>
<p>When provisioning a Cosmos DB account you need to select the API that the Cosmos DB will support. Only one API is supported per account and this value cannot be changed. Properties such as multi-region support and multi-master writes can be configured during account creation and can be modified later.</p>
<p>An account must have a unique name as this is used as part of the public URL to access the account (e.g. https://[Resource_group_name][Cosmos_DB_Account_Name][Region].documents.azure.com).</p>
<p>The account is the top level component of data organisation.</p>
<ul>
<li>The Account contains Databases (also known as Keyspaces in the Cassandra API).</li>
<li>Databases/Keyspaces contain Containers which are realised as Collections (Core API and MongoDB API), Tables (Cassandra API and Table API), or Graphs (Gremlin API)</li>
<li>Containers contain Items which are realised as Documents (Core API and MongoDB API), Rows (Cassandra API), Items (Table API), or Nodes and Edges (Gremlin API)</li>
<li>Containers can also contain stored procedures, user-defined functions, triggers, conflicts, and merge procedures (note this is <em>not</em> at the database level like with traditional SQL server)</li>
</ul>
<h3 id="creating-a-database-container">Creating a database container</h3>
<p>Accounts using provisioned throughput can contain up to <a href="https://docs.microsoft.com/en-us/azure/cosmos-db/concepts-limits" target="_blank">500 databases</a> and 500 containers (although only 25 containers per database can use shared throughput).</p>
<p>Accounts using serverless throughput can contain up to 500 containers.</p>
<h2 id="measuring-performance">Measuring Performance</h2>
<p>There are two main aspects to database performance: latency and throughput</p>
<ul>
<li>Latency: how fast is the response for a given request? To lower latency we can make sure service is close to the user.</li>
<li>Throughput: how many requests can be served within a specific period of time? Cosmos DB is designed to handle many workloads. The maximum throughput can be increased by specifying more <strong>Request units</strong> (for an increased cost).</li>
</ul>
<h3 id="throughput">Throughput</h3>
<p>Request Units (RU/s) are used to measure throughput. RUs represent a combination of memory, CPU and IOPs. RUs can be measured using the Data Explorer (see the Query Stats tab in the Results).</p>
<p>RUs can be provisioned at the database (shared) or dedicated for a particular container. The minimum number of RUs that can be provisioned is 400. The default maximum is 100,000 RUs (but this limit can be increased and is, theoretically, unlimited).
400 RUs is roughly $0.58 per hour.</p>
<p>If reserved throughput limits are exceeded than requests are throttled 
(APIs return a <code>429 - Too Many Requests</code> error). Multiple 429 errors indicate the provisioned limit should be increased.</p>
<p>It is recommended to set the throughput at the container level.</p>
<h2 id="horizontal-scaling">Horizontal Scaling</h2>
<p>Cosmos DB can store an unlimited amount of data.</p>
<p>A single CosmosDB container may be implemented by multiple physical machine (depending on the data storage requirements and provisioned RU throughput).</p>
<h2 id="partitioning">Partitioning</h2>
<p>Each container has a partition key. The partition key determines how the container divides data amongst the underlying physical machines. All data with a given partition key becomes part of the same logical partition (data is stored together). Multiple logical partitions may be stored on the same underlying physical machines. However this is internal to CosmosDB and is not something which we have control over.</p>
<ul>
<li>Partitioning: the items in a container are divided into distinct subsets called logical partitions.</li>
<li>Partition key is the value by which Azure organises your data into logical divisions.</li>
<li>Logical partitions are formed based on the value of a partition key this is associated with each item in a container.</li>
<li>Physical partitions: internally, one or more logical partitions are mapped to a single physical partition. A single logical partition <strong>cannot</strong> be divided into multiple physical partitions. Logical partitions can, over time, be moved from one physical partition to another - this migration is completely transparent.</li>
</ul>
<h3 id="avoiding-hot-partitions">Avoiding hot partitions</h3>
<p>Container RUs are divided between the logical partitions. RUs allocated to one partition cannot be used by other partitions. This means that if one partition is very active then it may reach the RU threshold and cause <code>429 - Too Many Requests</code> error even if RUs are available in other partitions.</p>
<p>To ensure logical partitions are used effectively, a partition key that evenly distributes data amongst partitions should be used. If one partition has the majority of the data then it will consume more RUs and may be rate limited - this is a 'hot' partition on store.</p>
<p>Similarly, we should try to ensure all queries can be distributed evenly across all available partitions. A query that just queries one partition can result in that partition consuming all available RUs and being rate limited - this is a 'hot' partition on throughput. Using a date as a partition key and having a system where most users are querying on the current day could cause a hot partition on throughput.</p>
<h3 id="single-partition-and-cross-partition-queries">Single partition and cross partition queries</h3>
<p>A single partition query is one where CosmosDB can infer (from the partition key of the container) that data matching the query being performed can be retrieved from a single partition. This is efficient.</p>
<p>A cross partition (or fan out) query requires CosmosDB to run a query across all partitions and combine the results. These should be avoided where possible.</p>
<h3 id="composite-keys">Composite keys</h3>
<p>CosmosDB has some limits with regards to data:</p>
<ul>
<li>A single document cannot exceed 2MB of data.</li>
<li>A single logical partition cannot exceed 20GB of data.</li>
</ul>
<p>Partition keys that result in data that exceeds these limits should be redesigned. One way to do this is to create a <code>composite key</code> from the underlying data that can be used as a partition key with a high degree of cardinality/variability which will result in a large number of logical partitions.</p>
<p>For example, in an application where we track web site activity we may be able to create a composite key that consists of the unique <code>userid</code> and the day that the user visited the site in <code>ddmmyyyy</code> format. This results in more, smaller logical partitions as we create data for the same user on different days in different partitions. This avoids the partition size limit and and allows us to scale further.</p>
<p>The main disadvantage of a highly cardinality partition key is that it can lead to more cross-partition queries.</p>
<h3 id="partition-key-best-practices">Partition key best practices</h3>
<ul>
<li>Evenly distribute storage
<ul>
<li>Make sure you pick you partition key that does not result in hot spots within the application</li>
<li>Have high cardinality</li>
<li>Use partition key with large number of values</li>
<li>Examples: user id, product id, GUID</li>
</ul>
</li>
<li>Evenly distribute request
<ul>
<li>RUs evenly distributed across all partitions</li>
<li>Review <code>where</code> clauses of top queries</li>
</ul>
</li>
<li>Consider document and partition limit while designed partition key
<ul>
<li>Max document size = 2MB</li>
<li>Max logical partition = 20GB</li>
</ul>
</li>
</ul>
<h2 id="time-to-live">Time to live</h2>
<p>CosmosDB has a time to live feature that can automatically delete data after a given period of time. Time to live is configured against each container's settings and has 3 possible settings:</p>
<ol>
<li>Off - items are not automatically deleted</li>
<li>On - items are deleted after the defined the number of seconds</li>
<li>On (no default) - items must define a <code>ttl</code> property and only items with a <code>ttl</code> are deleted (after <code>ttl</code> seconds)</li>
</ol>
<p>The time to live process works in the background and uses leftover RUs. Data deletion may be delayed if there is not enough RUs. However, even if the time to live process has not executed, queries will not return rows where the <code>ttl</code> has expired.</p>
<h2 id="global-distribution">Global Distribution</h2>
<p>CosmosDB allows data to be easily replicated to different data centres around the world. Simply go into the Azure portal and select the <code>Replicate Data globally</code>, then select the Azure data centres to replicate to. By default any new data centres will be a read replica.</p>
<p>Certain Azure data centres are <a href="https://docs.microsoft.com/en-us/azure/availability-zones/cross-region-replication-azure#azure-cross-region-replication-pairings-for-all-geographies" target="_blank">paired</a> (e.g. UK West and UK South). Regions are paired for cross-region replication based on proximity and other factors.</p>
<p>If we are replicating data for disaster recovery or business continuity purposes then we should choose centres that form a pair to maximise the benefits of the fast connections between these pairs.</p>
<p>If we are replicating data to reduce latency then we should replicate the data to the data centre that is closest to the user (even if this is not a paired region).</p>
<p>Costs for globally distributed CosmosDB increases with the number of replicates - 2 replicas costs twice the price of the primary database, 3 costs three times etc. You cannot replicate to a database that has a lower 'spec' than the primary.</p>

        </main>
    </div>

    <six-sided-footer></six-sided-footer>

    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.min.js" integrity="sha384-+YQ4JLhjyBLPDQt//I+STsc9iw4uQqACwlvpslubQzn4u2UU2UFM80nGisd026JF" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/prismjs@1.23.0/components/prism-core.min.js" integrity="sha256-2N+3bVl+vOCJyZ9ZbH9Eb99XKT/53oT5V8eRbB8bFcA=" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/prismjs@1.23.0/plugins/autoloader/prism-autoloader.min.js" integrity="sha256-33Qw0lN3qo7tLZL4c7vDLCapRUs+gNtQRaVIOHk4Ors=" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/mermaid@8.11.0/dist/mermaid.min.js"></script>
    
    <script>
        $(function() {
            mermaid.initialize({ startOnLoad: true, theme: 'dark' });
        });
    </script>
</body>
</html>
